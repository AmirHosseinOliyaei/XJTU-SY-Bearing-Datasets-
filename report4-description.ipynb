{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import math\n",
        "import random\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from scipy.stats import kurtosis\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from torch.optim.swa_utils import AveragedModel, SWALR\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "BASE_DIR = Path(r\"D:\\DRIVE\\UNI\\moviZ\\TERM8\\ANN-article--AzRiz\\OurWorks\\Data-description\\phase1\")\n",
        "REVERSED_FOLDER = \"reversed\"\n",
        "CACHE_DIR = Path(\"cache_article_ready_vfinal\")\n",
        "CACHE_DIR.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "TARGET_LEN = 2048\n",
        "SMOOTH_KERNEL = 7\n",
        "BATCH_SIZE = 64\n",
        "ACCUM_STEPS = 2\n",
        "EPOCHS = 300\n",
        "LR = 3e-4\n",
        "MIN_LR = 1e-6\n",
        "WARMUP_EPOCHS = 10\n",
        "WEIGHT_DECAY = 1e-4\n",
        "PATIENCE = 100\n",
        "VAL_RATIO = 0.15\n",
        "TEST_RATIO = 0.15\n",
        "SEED = 42\n",
        "AUGMENT = True\n",
        "DEBUG = False\n",
        "RUL_CLIP = 130\n",
        "USE_HUBER_LOSS = True\n",
        "USE_TTA = True\n",
        "SWA_START = 150"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div dir=\"rtl\">\n",
        "\n",
        "## Configurations\n",
        "\n",
        "این بخش متغیرهای ثابت و hyperparameterهای مدل را تعریف می‌کند، شامل مسیرهای directory برای داده‌ها و cache، طول سیگنال هدف (TARGET_LEN)، اندازه batch، تعداد epochs، learning rate، و flagهایی برای فعال‌سازی ویژگی‌هایی مثل data augmentation، Huber loss، و test-time augmentation (TTA).\n",
        "\n",
        "متغیرها با مقادیر مشخص initialize می‌شوند. Path از pathlib برای مدیریت مسیرهای فایل به‌صورت cross-platform استفاده می‌کند. دستور mkdir دایرکتوری cache را اگر وجود نداشته باشد ایجاد می‌کند.\n",
        "\n",
        "مسیرهای فایل و hyperparameterها تعریف می‌شوند. برای مثال، CACHE_DIR برای ذخیره dataهای preprocess‌شده تنظیم می‌شود، و BATCH_SIZE=64 برای بهینه‌سازی memory و سرعت train انتخاب می‌شود.\n",
        "\n",
        "تعریف متغیرها در یک مکان مرکزی، کد را modular و قابل تنظیم می‌کند. مثلاً تغییر LR یا EPOCHS بدون نیاز به ویرایش کل کد ممکن است. مقادیر مثل EPOCHS=300 برای اطمینان از convergence کافی مدل انتخاب شده‌اند، و SEED=42 برای reproducibility نتایج است. flagهایی مثل AUGMENT=True برای افزایش generalization مدل و کاهش overfitting فعال هستند.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div dir=\"rtl\">\n",
        "\n",
        "## Seed and Device Setup\n",
        "\n",
        "این بخش seed را برای libraryهای random، numpy و torch تنظیم می‌کند تا نتایج reproducible باشند و دستگاه محاسباتی (GPU یا CPU) را انتخاب می‌کند.\n",
        "\n",
        "تابع seed روی هر library اعمال می‌شود تا عملیات تصادفی مثل shuffling یا weight initialization یکسان باشند. اگر CUDA در دسترس باشد، تنظیمات اضافی برای determinism اعمال می‌شود. سپس device بر اساس در دسترس بودن GPU انتخاب می‌شود.\n",
        "\n",
        "ابتدا seedها تنظیم می‌شوند، سپس در دسترس بودن GPU بررسی می‌شود، و device مناسب (cuda یا cpu) allocate می‌شود. پیام چاپ‌شده device انتخاب‌شده را نشان می‌دهد.\n",
        "\n",
        "تنظیم seed برای اطمینان از reproducibility نتایج ضروری است، چون deep learning شامل عملیات تصادفی است. استفاده از GPU سرعت محاسبات را به شدت افزایش می‌دهد، به‌ویژه برای neural networkهای پیچیده. تنظیمات cudnn.deterministic از تغییرات تصادفی در GPU جلوگیری می‌کند تا نتایج در هر اجرا یکسان باشند.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def smooth_signal(x, kernel=SMOOTH_KERNEL):\n",
        "    if kernel is None or kernel <= 1:\n",
        "        return x.astype(np.float32, copy=False)\n",
        "    k = np.ones(kernel, dtype=np.float32) / kernel\n",
        "    return np.convolve(x.astype(np.float32, copy=False), k, mode='same')\n",
        "\n",
        "def downsample_avg(signal, target_len):\n",
        "    if len(signal) == target_len:\n",
        "        return signal.astype(np.float32, copy=False)\n",
        "    parts = np.array_split(signal, target_len)\n",
        "    return np.array([p.mean(dtype=np.float32) for p in parts], dtype=np.float32)\n",
        "\n",
        "def clip_signal(signal, clip_percentile=99.5):\n",
        "    clip_value = np.percentile(np.abs(signal), clip_percentile)\n",
        "    return np.clip(signal, -clip_value, clip_value)\n",
        "\n",
        "def frequency_augmentation(signal, scale_range=(0.85, 1.15)):\n",
        "    freq = np.fft.fft(signal)\n",
        "    scale = np.random.uniform(*scale_range)\n",
        "    freq = freq * scale\n",
        "    return np.fft.ifft(freq).real.astype(np.float32)\n",
        "\n",
        "def mixup_data(x, y, alpha=0.2):\n",
        "    if alpha > 0:\n",
        "        lam = np.random.beta(alpha, alpha)\n",
        "    else:\n",
        "        lam = 1\n",
        "    batch_size = x.size(0)\n",
        "    index = torch.randperm(batch_size)\n",
        "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
        "    y_a, y_b = y, y[index]\n",
        "    return mixed_x, y_a, y_b, lam"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div dir=\"rtl\">\n",
        "\n",
        "## Preprocessing Functions\n",
        "\n",
        "این بخش functionهایی را برای preprocess سیگنال‌ها تعریف می‌کند: smooth_signal برای کاهش نویز با میانگین متحرک، downsample_avg برای کاهش طول سیگنال به target_len، clip_signal برای محدود کردن مقادیر extreme، frequency_augmentation برای تغییر مقیاس در domain فرکانس، و mixup_data برای ترکیب sampleها در train.\n",
        "\n",
        "هر function یک سیگنال ورودی می‌گیرد و آن را transform می‌کند. مثلاً smooth_signal از convolution با یک uniform kernel برای averaging استفاده می‌کند، و mixup_data دو sample را با وزن تصادفی ترکیب می‌کند.\n",
        "\n",
        "سیگنال به float32 تبدیل می‌شود، عملیات مورد نظر (مثل convolution، FFT، یا interpolation) اعمال می‌شود، و خروجی با type مناسب بازگشت داده می‌شود. برای mixup، دو نمونه با نسبت lam ترکیب می‌شوند.\n",
        "\n",
        "این functionها سیگنال‌ها را برای input به مدل آماده می‌کنند. smoothing نویزهای کوچک را حذف می‌کند، downsampling طول سیگنال را استاندارد می‌کند، clipping از تأثیر outliers جلوگیری می‌کند، و augmentationها (frequency و mixup) تنوع data را افزایش می‌دهند تا model بهتر generalize کند و overfitting کاهش یابد.\n",
        "\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_all_files(base_dir):\n",
        "    files = list(base_dir.rglob(f\"*/{REVERSED_FOLDER}/*.csv\"))\n",
        "    return sorted([f for f in files if f.is_file()])\n",
        "\n",
        "def find_degradation_start(signals):\n",
        "    kurt = np.array([kurtosis(s) for s in signals])\n",
        "    kurt = smooth_signal(kurt, kernel=5)\n",
        "    normal_end = min(50, len(kurt) // 5)\n",
        "    mean_normal = np.mean(kurt[:normal_end])\n",
        "    std_normal = np.std(kurt[:normal_end])\n",
        "    threshold = mean_normal + 3 * std_normal\n",
        "    for i in range(normal_end, len(kurt)):\n",
        "        if kurt[i] > threshold:\n",
        "            return i\n",
        "    return len(kurt) // 2\n",
        "\n",
        "def preprocess_and_cache(files):\n",
        "    print(\"Preprocessing & Caching (once)...\")\n",
        "    X_list, y_list = [], []\n",
        "    max_rul = 0.0\n",
        "    for f in tqdm(files):\n",
        "        df = pd.read_csv(f, header=None, dtype=np.float32)\n",
        "        sigs = df.iloc[:, :-1].values\n",
        "        labels = df.iloc[:, -1].values.astype(np.float32)\n",
        "        if labels.size == 0:\n",
        "            continue\n",
        "        deg_start = find_degradation_start(sigs)\n",
        "        rul_at_start = labels[deg_start]\n",
        "        adjusted_labels = labels.copy()\n",
        "        adjusted_labels[:deg_start] = rul_at_start\n",
        "        adjusted_labels = np.clip(adjusted_labels, 0, RUL_CLIP)\n",
        "        max_rul = max(max_rul, float(adjusted_labels.max()))\n",
        "        for sig, lab in zip(sigs, adjusted_labels):\n",
        "            sig = clip_signal(sig)\n",
        "            if SMOOTH_KERNEL > 1:\n",
        "                sig = smooth_signal(sig, SMOOTH_KERNEL)\n",
        "            sig_ds = downsample_avg(sig, TARGET_LEN)\n",
        "            X_list.append(sig_ds)\n",
        "            y_list.append(lab)\n",
        "    X = np.asarray(X_list, dtype=np.float32)\n",
        "    y = np.asarray(y_list, dtype=np.float32)\n",
        "    np.save(CACHE_DIR / \"X_raw.npy\", X)\n",
        "    np.save(CACHE_DIR / \"y.npy\", y)\n",
        "    np.save(CACHE_DIR / \"max_rul.npy\", np.array([max_rul], dtype=np.float32))\n",
        "    print(f\"Cached raw: X={X.shape}, y={y.shape}, max_rul={max_rul:.2f}\")\n",
        "\n",
        "def load_cached_raw():\n",
        "    X = np.load(CACHE_DIR / \"X_raw.npy\")\n",
        "    y = np.load(CACHE_DIR / \"y.npy\")\n",
        "    max_rul = float(np.load(CACHE_DIR / \"max_rul.npy\")[0])\n",
        "    return X, y, max_rul\n",
        "\n",
        "def split_indices(n_samples, test_ratio=TEST_RATIO, val_ratio=VAL_RATIO, seed=SEED):\n",
        "    idx = np.arange(n_samples)\n",
        "    rng = np.random.RandomState(seed)\n",
        "    rng.shuffle(idx)\n",
        "    n_test = int(test_ratio * n_samples)\n",
        "    n_val = int(val_ratio * (n_samples - n_test))\n",
        "    return idx[n_test + n_val:], idx[n_test:n_test + n_val], idx[:n_test]\n",
        "\n",
        "def compute_train_norm_stats(X_train):\n",
        "    mean = X_train.mean(axis=0, dtype=np.float64).astype(np.float32)\n",
        "    std = X_train.std(axis=0, dtype=np.float64).astype(np.float32)\n",
        "    std[std < 1e-6] = 1.0\n",
        "    return mean, std"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div dir=\"rtl\">\n",
        "\n",
        "## Data Loading and Preprocessing Functions\n",
        "\n",
        "این بخش شامل functionهایی برای یافتن فایل‌های CSV، تشخیص نقطه شروع degradation با kurtosis، preprocess و cache کردن data، load کردن dataهای cached، تقسیم indices برای train/validation/test، و محاسبه mean و std برای normalization.\n",
        "\n",
        "get_all_files تمام فایل‌های CSV در directory مشخص را پیدا می‌کند. find_degradation_start از kurtosis برای شناسایی شروع تخریب استفاده می‌کند. preprocess_and_cache داده‌ها را می‌خواند، سیگنال‌ها را preprocess می‌کند (clip، smooth، downsample)، labels را تنظیم می‌کند، و در فایل‌های npy ذخیره می‌کند. load_cached_raw این فایل‌ها را می‌خواند. split_indices داده‌ها را به سه بخش تقسیم می‌کند، و compute_train_norm_stats آماری برای normalization محاسبه می‌کند.\n",
        "\n",
        "فایل‌ها اسکن می‌شوند، data خوانده و transform می‌شود، labels بر اساس degradation point تنظیم و clipped می‌شوند، max_rul محاسبه می‌شود، و data ذخیره می‌شود. در load، فایل‌های npy خوانده می‌شوند. indices به‌صورت random با seed تقسیم می‌شوند، و mean/std محاسبه می‌شوند.\n",
        "\n",
        "caching سرعت را افزایش می‌دهد، چون preprocess فقط یک بار انجام می‌شود. تشخیص degradation point با kurtosis labels را دقیق‌تر می‌کند، چون قبل از تخریب RUL ثابت فرض می‌شود. تقسیم data برای train، validation، و test ضروری است تا مدل ارزیابی شود. normalization مقیاس featureها را متعادل می‌کند تا training سریع‌تر و پایدارتر باشد.\n",
        "\n",
        "- **سقف‌گذاری (Clipping) برچسب‌ها (RUL)**:\n",
        "  - برچسب‌های RUL با استفاده از `np.clip` به بازه [۰, ۱۳۰] محدود می‌شوند (RUL_CLIP=۱۳۰).\n",
        "  - **دلیل استفاده**: سقف‌گذاری RUL برای جلوگیری از تأثیر مقادیر بسیار بزرگ RUL (که ممکن است نویزی یا غیرقابل اعتماد باشند) بر عملکرد مدل است. این تکنیک به مدل کمک می‌کند تا روی مراحل نزدیک به شکست تمرکز کند، بیش‌برازش را کاهش دهد و دقت پیش‌بینی در مراحل بحرانی را افزایش دهد. این روش در پیش‌بینی RUL برای جلوگیری از overestimation و بهبود تعمیم‌پذیری مدل رایج است.\n",
        "  - **نمونه مقالات استفاده‌کننده از سقف‌گذاری در دیتاست XJTU-SY**:\n",
        "    - Ding et al. (۲۰۲۲) در مقاله \"Prediction of remaining useful life of rolling bearing based on fractal dimension and convolutional neural network\" (منتشر در Measurement and Control) از clipping RUL در دیتاست XJTU-SY برای بهبود دقت پیش‌بینی CNN استفاده کرده‌اند.\n",
        "    - Lin (۲۰۲۱) در مقاله \"A Novel Approach of Label Construction for Predicting Remaining Useful Life of Machinery\" (منتشر در Shock and Vibration) از روش‌های clipping و تنظیم piecewise linear در XJTU-SY bearing data برای بهبود label construction و RUL prediction بهره برده‌اند.\n",
        "    - Wang et al. (۲۰۲۲) در مقاله \"RUL prediction of rolling bearings based on improved variational mode decomposition and LSTM\" (منتشر در Advances in Mechanical Engineering) از capping RUL در XJTU-SY برای تمرکز روی مراحل تخریب و کاهش خطا در مدل LSTM استفاده کرده‌اند.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BearingDataset(Dataset):\n",
        "    def __init__(self, X, y, max_rul, mean, std, train=False, augment=AUGMENT):\n",
        "        self.X_raw = X\n",
        "        self.y_raw = y\n",
        "        self.y = (y / max_rul).astype(np.float32)\n",
        "        self.max_rul = max_rul\n",
        "        self.mean, self.std = mean, std\n",
        "        self.augment = augment and train\n",
        "        self.noise_scale = 0.015\n",
        "        self.shift_max = 12\n",
        "        self.scale_range = (0.85, 1.15)\n",
        "        self.freq_scale_range = (0.8, 1.2)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X_raw)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.X_raw[idx].copy()\n",
        "        if self.augment:\n",
        "            noise = np.random.normal(0, self.noise_scale * self.std, size=x.shape).astype(np.float32)\n",
        "            x = x + noise\n",
        "            shift = np.random.randint(-self.shift_max, self.shift_max)\n",
        "            x = np.roll(x, shift)\n",
        "            scale = np.random.uniform(*self.scale_range)\n",
        "            x = x * scale\n",
        "            if np.random.random() < 0.5:\n",
        "                x = frequency_augmentation(x, self.freq_scale_range)\n",
        "        x = ((x - self.mean) / self.std).astype(np.float32)\n",
        "        weight = max(1.0, (self.max_rul - self.y_raw[idx]) / 50.0)\n",
        "        return torch.from_numpy(x).unsqueeze(0), torch.tensor(self.y[idx], dtype=torch.float32), torch.tensor(weight, dtype=torch.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div dir=\"rtl\">\n",
        "\n",
        "## Dataset Class\n",
        "\n",
        "کلاس BearingDataset یک PyTorch Dataset برای مدیریت data تعریف می‌کند. labels را normalize می‌کند (تقسیم بر max_rul)، و در حالت train augmentationهایی مثل اضافه کردن noise، shift، scaling، و frequency augmentation اعمال می‌کند.\n",
        "\n",
        "در __init__، data و labels ذخیره و normalize می‌شوند. در __getitem__، یک sample انتخاب می‌شود، اگر augment فعال باشد، transformهای تصادفی اعمال می‌شوند، سپس data normalize شده و به tensor تبدیل می‌شود. weight برای هر sample محاسبه می‌شود.\n",
        "\n",
        "نمونه انتخاب می‌شود، کپی از data خام گرفته می‌شود، augmentationها (noise، shift، scale، frequency) با احتمال مشخص اعمال می‌شوند، data با mean و std normalize می‌شود، و خروجی به صورت tensor با weight بازگشت داده می‌شود.\n",
        "\n",
        "این کلاس امکان batch loading با DataLoader را فراهم می‌کند. augmentation تنوع data را افزایش می‌دهد تا model روی dataهای unseen بهتر عمل کند. normalization مقیاس featureها را یکسان می‌کند تا gradient descent پایدارتر باشد. weightها برای تمرکز بیشتر روی sampleهایی که RUL کمتری دارند (نزدیک به failure) استفاده می‌شوند تا خطاها در این نقاط حساس کاهش یابد.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SEBlock(nn.Module):\n",
        "    def __init__(self, channel, reduction=8):\n",
        "        super(SEBlock, self).__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(channel, channel // reduction, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(channel // reduction, channel, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, _ = x.size()\n",
        "        y = self.avg_pool(x).view(b, c)\n",
        "        y = self.fc(y).view(b, c, 1)\n",
        "        return x * y.expand_as(x)\n",
        "\n",
        "class OptimizedModel(nn.Module):\n",
        "    def __init__(self, input_len=TARGET_LEN):\n",
        "        super().__init__()\n",
        "        self.conv1_large = nn.Sequential(\n",
        "            nn.Conv1d(1, 32, kernel_size=15, padding=7),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.BatchNorm1d(32, momentum=0.1),\n",
        "            nn.Dropout1d(0.1),\n",
        "            nn.MaxPool1d(3)\n",
        "        )\n",
        "        self.conv1_small = nn.Sequential(\n",
        "            nn.Conv1d(1, 32, kernel_size=5, padding=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.BatchNorm1d(32, momentum=0.1),\n",
        "            nn.Dropout1d(0.1),\n",
        "            nn.MaxPool1d(3)\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv1d(64, 128, kernel_size=7, padding=3),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.BatchNorm1d(128, momentum=0.1),\n",
        "            nn.Dropout1d(0.15),\n",
        "            nn.MaxPool1d(3)\n",
        "        )\n",
        "        self.se_block = SEBlock(128, reduction=8)\n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv1d(128, 192, kernel_size=5, padding=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.BatchNorm1d(192, momentum=0.1),\n",
        "            nn.Dropout1d(0.2),\n",
        "            nn.MaxPool1d(3)\n",
        "        )\n",
        "        self.conv3_extra = nn.Sequential(\n",
        "            nn.Conv1d(192, 256, kernel_size=5, padding=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.BatchNorm1d(256, momentum=0.1),\n",
        "            nn.Dropout1d(0.25),\n",
        "            nn.MaxPool1d(3)\n",
        "        )\n",
        "        self.conv4 = nn.Sequential(\n",
        "            nn.Conv1d(256, 320, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.BatchNorm1d(320, momentum=0.1),\n",
        "            nn.AdaptiveAvgPool1d(16)\n",
        "        )\n",
        "\n",
        "        with torch.no_grad():\n",
        "            dummy = torch.zeros(1, 1, input_len)\n",
        "            out1 = self.conv1_large(dummy)\n",
        "            out2 = self.conv1_small(dummy)\n",
        "            out = torch.cat([out1, out2], dim=1)\n",
        "            out = self.conv2(out)\n",
        "            out = self.se_block(out)\n",
        "            out = self.conv3(out)\n",
        "            out = self.conv3_extra(out)\n",
        "            out = self.conv4(out)\n",
        "            output_length = out.size(2)\n",
        "            output_channels = out.size(1)\n",
        "\n",
        "        total_downsample = input_len // output_length\n",
        "        self.shortcut = nn.Sequential(\n",
        "            nn.Conv1d(1, output_channels, kernel_size=1, stride=total_downsample),\n",
        "            nn.BatchNorm1d(output_channels, momentum=0.1)\n",
        "        )\n",
        "\n",
        "        self.gru = nn.GRU(input_size=320, hidden_size=160, num_layers=2, batch_first=True, bidirectional=True, dropout=0.3)\n",
        "        self.attn = nn.MultiheadAttention(embed_dim=320, num_heads=8, dropout=0.15, batch_first=True)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(320, 320),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.LayerNorm(320),\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(320, 192),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.LayerNorm(192),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(192, 96),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.LayerNorm(96),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(96, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = self.shortcut(x)\n",
        "        x1 = self.conv1_large(x)\n",
        "        x2 = self.conv1_small(x)\n",
        "        x = torch.cat([x1, x2], dim=1)\n",
        "        x = self.conv2(x)\n",
        "        x = self.se_block(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.conv3_extra(x)\n",
        "        x = self.conv4(x)\n",
        "        x = x + residual\n",
        "        x = x.permute(0, 2, 1)\n",
        "        x, _ = self.gru(x)\n",
        "        x = self.attn(x, x, x)[0]\n",
        "        x = x.mean(dim=1)\n",
        "        x = self.classifier(x)\n",
        "        return x.squeeze(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div dir=\"rtl\">\n",
        "\n",
        "## Model\n",
        "\n",
        "کلاس SEBlock یک squeeze-and-excitation block برای channel attention پیاده‌سازی می‌کند. کلاس OptimizedModel یک neural network پیچیده با convolutional layers چندمقیاس، SE block، residual connection، GRU برای sequence modeling، multi-head attention، و یک classifier چندلایه تعریف می‌کند.\n",
        "\n",
        "در __init__، لایه‌ها تعریف می‌شوند. یک dummy input برای محاسبه output size لایه‌های conv استفاده می‌شود تا shortcut layer هماهنگ شود. در forward، input از conv layers، SE block، GRU، attention و classifier عبور می‌کند و output تک‌بعدی تولید می‌شود.\n",
        "\n",
        "input ابتدا از دو مسیر conv (large و small kernel) پردازش می‌شود، outputs ترکیب می‌شوند، سپس از conv layers بعدی، SE block، و residual connection عبور می‌کند. بعد از permute، GRU توالی را پردازش می‌کند، attention featureها را وزن‌دهی می‌کند، و classifier RUL را پیش‌بینی می‌کند.\n",
        "\n",
        "این architecture برای استخراج featureهای local (با conv) و global (با GRU و attention) از سیگنال‌های temporal طراحی شده است. multi-scale convها الگوهای مختلف را capture می‌کنند، SE block مهم‌ترین channelها را highlight می‌کند، residual connection از vanishing gradient جلوگیری می‌کند، و GRU/attention وابستگی‌های زمانی را مدل می‌کنند. این ترکیب برای پیش‌بینی دقیق RUL بهینه شده است.\n",
        "\n",
        "##  تعریف مدل\n",
        "\n",
        "مدل یک شبکه کانولوشنی یک‌بعدی (1D CNN) بهینه‌شده با GRU و مکانیزم توجه شامل موارد زیر است:\n",
        "\n",
        "### لایه‌های کانولوشنی\n",
        "- **conv1_large**:\n",
        "  - ۳۲ فیلتر، اندازه هسته ۱۵، پدینگ ۷\n",
        "  - MaxPooling با اندازه ۳، Dropout ۰.۱\n",
        "- **conv1_small**:\n",
        "  - ۳۲ فیلتر، اندازه هسته ۵، پدینگ ۲\n",
        "  - MaxPooling با اندازه ۳، Dropout ۰.۱\n",
        "- **ترکیب**:\n",
        "  - خروجی `conv1_large` و `conv1_small` با `torch.cat` ترکیب شده و به ۶۴ کانال می‌رسد.\n",
        "- **conv2**:\n",
        "  - ۱۲۸ فیلتر، هسته ۷، پدینگ ۳\n",
        "  - MaxPooling با اندازه ۳، Dropout ۰.۱۵\n",
        "- **conv3**:\n",
        "  - ۱۹۲ فیلتر، هسته ۵، پدینگ ۲\n",
        "  - MaxPooling با اندازه ۳، Dropout ۰.۲\n",
        "- **conv3_extra**:\n",
        "  - ۲۵۶ فیلتر، هسته ۵، پدینگ ۲\n",
        "  - MaxPooling با اندازه ۳، Dropout ۰.۲۵\n",
        "- **conv4**:\n",
        "  - ۳۲۰ فیلتر، هسته ۳، پدینگ ۱\n",
        "  - `AdaptiveAvgPool1d` به طول ۱۶\n",
        "\n",
        "### بلوک Squeeze-and-Excitation (SE)\n",
        "- پس از `conv2`:\n",
        "  - کاهش ۸ کانالی برای تنظیم پویای کانال‌ها\n",
        "\n",
        "### Shortcut\n",
        "- مسیر مستقیم با کانولوشن ۱×۱ و BatchNorm برای ادغام با خروجی اصلی\n",
        "\n",
        "### لایه GRU\n",
        "- اندازه ورودی: ۳۲۰\n",
        "- اندازه پنهان: ۱۶۰\n",
        "- تعداد لایه‌ها: ۲\n",
        "- دوجهته\n",
        "- Dropout: ۰.۳\n",
        "\n",
        "### مکانیزم توجه (Attention)\n",
        "- اندازه ورودی: ۳۲۰\n",
        "- تعداد سرهای توجه: ۸\n",
        "- Dropout: ۰.۱۵\n",
        "\n",
        "### لایه‌های Fully Connected\n",
        "- لایه‌های خطی:\n",
        "  - ۳۲۰ → ۳۲۰ → ۱۹۲ → ۹۶ → ۱\n",
        "  - هر لایه با ReLU، LayerNorm و Dropout (نسبت‌های ۰.۵، ۰.۴، ۰.۳، ۰.۲)\n",
        "\n",
        "### جریان داده\n",
        "- خروجی‌های کانولوشنی با اتصال کوتاه جمع شده، سپس به GRU منتقل شده، با مکانیزم توجه پردازش شده و در نهایت به طبقه‌بندی‌کننده ارسال می‌شوند.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def smape_loss(pred, y):\n",
        "    return torch.mean(2 * torch.abs(pred - y) / (torch.abs(pred) + torch.abs(y) + 1e-8))\n",
        "\n",
        "def huber_loss(pred, y, delta=1.0):\n",
        "    err = pred - y\n",
        "    abs_err = torch.abs(err)\n",
        "    quadratic = torch.clamp(abs_err, max=delta)\n",
        "    linear = abs_err - quadratic\n",
        "    return torch.mean(0.5 * quadratic**2 + delta * linear)\n",
        "\n",
        "def fit(model, train_loader, val_loader, max_epochs, base_lr, min_lr, warmup_epochs, weight_decay, patience, device, swa_start):\n",
        "    if USE_HUBER_LOSS:\n",
        "        criterion = huber_loss\n",
        "    else:\n",
        "        criterion = nn.MSELoss()\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=base_lr, weight_decay=weight_decay)\n",
        "    scheduler = CosineAnnealingLR(optimizer, T_max=max_epochs - warmup_epochs, eta_min=min_lr)\n",
        "    swa_model = AveragedModel(model)\n",
        "    swa_scheduler = SWALR(optimizer, swa_lr=base_lr * 0.1)\n",
        "\n",
        "    model = model.to(device)\n",
        "    best_val = float('inf')\n",
        "    best_ep = -1\n",
        "    history = {\"train\": [], \"val\": [], \"val_smape\": []}\n",
        "\n",
        "    for epoch in range(1, max_epochs + 1):\n",
        "        print(f\"\\n===== Epoch {epoch}/{max_epochs} =====\")\n",
        "        if epoch <= warmup_epochs:\n",
        "            lr = base_lr * (epoch / warmup_epochs)\n",
        "            for param_group in optimizer.param_groups:\n",
        "                param_group['lr'] = lr\n",
        "\n",
        "        model.train()\n",
        "        train_loss, n = 0, 0\n",
        "        optimizer.zero_grad()\n",
        "        for i, (X, y, w) in enumerate(tqdm(train_loader, desc=\"Train\", leave=False)):\n",
        "            X, y, w = X.to(device), y.to(device), w.to(device)\n",
        "            if np.random.random() < 0.4:\n",
        "                X, y_a, y_b, lam = mixup_data(X, y, alpha=0.2)\n",
        "                pred = model(X)\n",
        "                if USE_HUBER_LOSS:\n",
        "                    loss_a = huber_loss(pred, y_a)\n",
        "                    loss_b = huber_loss(pred, y_b)\n",
        "                    mse_loss = lam * loss_a + (1 - lam) * loss_b\n",
        "                else:\n",
        "                    mse_loss = lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
        "                smape = lam * smape_loss(pred, y_a) + (1 - lam) * smape_loss(pred, y_b)\n",
        "            else:\n",
        "                pred = model(X)\n",
        "                if USE_HUBER_LOSS:\n",
        "                    mse_loss = huber_loss(pred, y)\n",
        "                else:\n",
        "                    mse_loss = criterion(pred, y)\n",
        "                smape = smape_loss(pred, y)\n",
        "\n",
        "            l1_loss = torch.mean(torch.abs(pred - y)) * 0.02\n",
        "            loss = mse_loss + 0.03 * smape + l1_loss\n",
        "            loss = loss / ACCUM_STEPS\n",
        "            loss.backward()\n",
        "\n",
        "            if (i + 1) % ACCUM_STEPS == 0 or (i + 1) == len(train_loader):\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "            train_loss += loss.item() * X.size(0) * ACCUM_STEPS\n",
        "            n += X.size(0)\n",
        "\n",
        "        train_loss /= n\n",
        "\n",
        "        model.eval()\n",
        "        val_loss, val_smape, n = 0, 0, 0\n",
        "        with torch.no_grad():\n",
        "            for X, y, _ in tqdm(val_loader, desc=\"Val\", leave=False):\n",
        "                X, y = X.to(device), y.to(device)\n",
        "                pred = model(X)\n",
        "                if USE_HUBER_LOSS:\n",
        "                    loss = huber_loss(pred, y)\n",
        "                else:\n",
        "                    loss = criterion(pred, y)\n",
        "                val_loss += loss.item() * X.size(0)\n",
        "                val_smape += (2 * torch.abs(pred - y) / (torch.abs(pred) + torch.abs(y) + 1e-8)).mean().item() * X.size(0)\n",
        "                n += X.size(0)\n",
        "        val_loss /= n\n",
        "        val_smape = 100 * val_smape / n\n",
        "\n",
        "        history[\"train\"].append(train_loss)\n",
        "        history[\"val\"].append(val_loss)\n",
        "        history[\"val_smape\"].append(val_smape)\n",
        "        print(f\"Train={train_loss:.4f} | Val={val_loss:.4f} | Val sMAPE={val_smape:.2f}%\")\n",
        "\n",
        "        if epoch > warmup_epochs:\n",
        "            if epoch < swa_start:\n",
        "                scheduler.step()\n",
        "            else:\n",
        "                swa_model.update_parameters(model)\n",
        "                swa_scheduler.step()\n",
        "\n",
        "        if val_loss < best_val - 1e-5:\n",
        "            best_val = val_loss\n",
        "            best_ep = epoch\n",
        "            torch.save(model.state_dict(), CACHE_DIR / \"best_model.pth\")\n",
        "            print(\"(best).\")\n",
        "        elif epoch - best_ep >= patience:\n",
        "            print(\"Early stopping.\")\n",
        "            break\n",
        "\n",
        "    torch.save(swa_model.state_dict(), CACHE_DIR / \"swa_model.pth\")\n",
        "    return history, swa_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div dir=\"rtl\">\n",
        "\n",
        "## Loss and Training Functions\n",
        "\n",
        "این بخش دو loss function تعریف می‌کند: sMAPE برای محاسبه خطای نسبی و Huber loss برای مقاومت در برابر outliers. تابع fit فرایند train مدل را مدیریت می‌کند با استفاده از AdamW optimizer، cosine annealing scheduler، stochastic weight averaging (SWA)، mixup augmentation، gradient accumulation، و early stopping.\n",
        "\n",
        "sMAPE خطای نسبی را محاسبه می‌کند، Huber loss ترکیبی از MSE و MAE است. در fit، model به device منتقل می‌شود، optimizer و scheduler تنظیم می‌شوند، و در هر epoch، data از train_loader خوانده می‌شود، loss محاسبه می‌شود (با mixup در 40% موارد)، gradientها accumulate و clip می‌شوند، و validation loss و sMAPE محاسبه می‌شوند.\n",
        "\n",
        "model به device می‌رود، loop روی epochs اجرا می‌شود. در warmup، learning rate به‌تدریج افزایش می‌یابد. در هر batch، mixup با احتمال اعمال می‌شود، loss ترکیبی (MSE/Huber + sMAPE + L1) محاسبه و backward می‌شود. gradientها پس از ACCUM_STEPS step می‌شوند. validation metrics محاسبه و history ذخیره می‌شود. اگر val_loss بهبود یابد، model ذخیره می‌شود، و SWA بعد از swa_start فعال می‌شود.\n",
        "\n",
        "Huber loss نسبت به MSE در برابر outliers مقاوم‌تر است، و sMAPE خطای نسبی را برای ارزیابی بهتر فراهم می‌کند. fit از warmup برای stability اولیه، gradient accumulation برای effective batch size بزرگ‌تر، SWA برای generalization بهتر، و early stopping برای جلوگیری از overfitting استفاده می‌کند. ترکیب lossها دقت model را در نقاط حساس بهبود می‌دهد.\n",
        "\n",
        "## ۳. یادگیری مدل\n",
        "\n",
        "### بهینه‌سازی\n",
        "- بهینه‌ساز: **AdamW**\n",
        "  - نرخ یادگیری اولیه: `3e-4`\n",
        "  - حداقل نرخ یادگیری: `1e-6`\n",
        "  - گرم‌کردن برای ۱۰ Epoch اول\n",
        "  - وزن‌دهی: `1e-4`\n",
        "- تابع هزینه: **Huber Loss** با دلتا ۱.۰ (برای مقاومت در برابر نویز) همراه با:\n",
        "  - sMAPE با وزن ۰.۰۳\n",
        "  - L1 Loss با وزن ۰.۰۲\n",
        "- محدود کردن گرادیان: حداکثر نورم ۰.۵\n",
        "- انباشت گرادیان: با ACCUM_STEPS=۲\n",
        "\n",
        "### داده‌افزایی در آموزش\n",
        "- **Mixup**:\n",
        "  - آلفا: ۰.۲\n",
        "  - احتمال: ۴۰٪\n",
        "\n",
        "### برنامه‌ریزی نرخ یادگیری\n",
        "- **CosineAnnealingLR**:\n",
        "  - T_max: ۲۹۰ (EPOCHS - WARMUP_EPOCHS)\n",
        "  - حداقل نرخ: `1e-6`\n",
        "- **Stochastic Weight Averaging (SWA)**: شروع از Epoch ۱۵۰ با نرخ یادگیری ۰.۱ برابر پایه\n",
        "\n",
        "### (Early Stopping)\n",
        "- توقف آموزش پس از ۱۰۰ Epoch بدون بهبود.\n",
        "\n",
        "### حفظ بهترین مدل\n",
        "- ذخیره در: `best_model.pth` (بر اساس کمترین خطای اعتبارسنجی) و `swa_model.pth` برای مدل SWA\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tta_predict(model, x, n_aug=7):\n",
        "    model.eval()\n",
        "    preds = []\n",
        "    with torch.no_grad():\n",
        "        for _ in range(n_aug):\n",
        "            x_aug = x.clone()\n",
        "            if np.random.random() < 0.6:\n",
        "                noise = torch.randn_like(x_aug) * 0.01\n",
        "                x_aug += noise\n",
        "            if np.random.random() < 0.5:\n",
        "                shift = np.random.randint(-5, 5)\n",
        "                x_aug = torch.roll(x_aug, shifts=shift, dims=-1)\n",
        "            if np.random.random() < 0.5:\n",
        "                scale = np.random.uniform(0.95, 1.05)\n",
        "                x_aug = x_aug * scale\n",
        "            pred = model(x_aug)\n",
        "            preds.append(pred)\n",
        "    return torch.stack(preds).mean(dim=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div dir=\"rtl\">\n",
        "\n",
        "## Test-Time Augmentation (TTA)\n",
        "\n",
        "این تابع test-time augmentation را پیاده‌سازی می‌کند، که در آن input چندین بار با transformهای تصادفی (noise، shift، scale) پیش‌بینی می‌شود و میانگین پیش‌بینی‌ها گرفته می‌شود.\n",
        "\n",
        "model به حالت eval می‌رود، input کلون می‌شود، transformهای تصادفی با احتمال مشخص اعمال می‌شوند، پیش‌بینی برای هر نسخه گرفته می‌شود، و میانگین پیش‌بینی‌ها محاسبه می‌شود.\n",
        "\n",
        "input چندین بار (n_aug=7) با تغییرات مثل اضافه کردن noise، shift زمانی، یا scaling پردازش می‌شود. هر نسخه از model پیش‌بینی می‌شود، و outputs در tensor stack شده و mean گرفته می‌شود.\n",
        "\n",
        "TTA با averaging پیش‌بینی‌های چند نسخه augmented، variance پیش‌بینی را کاهش می‌دهد و accuracy را بدون نیاز به retraining افزایش می‌دهد. transformهای انتخاب‌شده مشابه augmentationهای train هستند تا model روی آنها robust باشد.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_test(model, test_loader, max_rul, use_swa=False, swa_model=None):\n",
        "    if use_swa and swa_model is not None:\n",
        "        model = swa_model\n",
        "        model = model.to(device)  \n",
        "        model.eval()\n",
        "        print(\"Using SWA model for evaluation...\")\n",
        "    else:\n",
        "        model.load_state_dict(torch.load(CACHE_DIR / \"best_model.pth\", map_location=device))\n",
        "        model = model.to(device)\n",
        "        print(\"Using best model for evaluation...\")\n",
        "\n",
        "    model.eval()\n",
        "    preds, trues = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(test_loader, desc=\"Test\", leave=False):\n",
        "            X, y, _ = batch\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            if USE_TTA:\n",
        "                pred = tta_predict(model, X, n_aug=7)\n",
        "            else:\n",
        "                pred = model(X)\n",
        "            pred = pred.cpu().numpy() * max_rul\n",
        "            true = y.cpu().numpy() * max_rul\n",
        "            preds.extend(pred.tolist())\n",
        "            trues.extend(true.tolist())\n",
        "\n",
        "    preds, trues = np.array(preds), np.array(trues)\n",
        "    mse = mean_squared_error(trues, preds)\n",
        "    mae = mean_absolute_error(trues, preds)\n",
        "    rmse = math.sqrt(mse)\n",
        "    r2 = r2_score(trues, preds)\n",
        "    smape = 100 * np.mean(2 * np.abs(preds - trues) / (np.abs(preds) + np.abs(trues) + 1e-8))\n",
        "    mae_pct = 100 * mae / (trues.max() if trues.max() > 0 else 1.0)\n",
        "\n",
        "    print(f\"\\n\" + \"=\"*60)\n",
        "    print(f\"FINAL TEST RESULTS (RUL_CLIP={RUL_CLIP}):\")\n",
        "    print(f\"MSE={mse:.2f} | MAE={mae:.2f} | RMSE={rmse:.2f}\")\n",
        "    print(f\"R²={r2:.4f} | sMAPE={smape:.2f}% | MAE%Max={mae_pct:.2f}%\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.scatter(trues, preds, alpha=0.6, color='#1f77b4', edgecolors='w', linewidth=0.5, label=\"Predictions\")\n",
        "    plt.plot([trues.min(), trues.max()], [trues.min(), trues.max()], 'r--', lw=2, label=\"Ideal Fit\")\n",
        "    plt.xlabel(\"True RUL\", fontsize=12)\n",
        "    plt.ylabel(\"Predicted RUL\", fontsize=12)\n",
        "    plt.title(\"Predicted vs True RUL (Optimized Model — Article Ready)\", fontsize=14, fontweight='bold')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return preds, trues"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div dir=\"rtl\">\n",
        "\n",
        "## Test Evaluation\n",
        "\n",
        "این تابع model را روی test data ارزیابی می‌کند، metrics مثل MSE، MAE، RMSE، R2، sMAPE و MAE%Max را محاسبه می‌کند، و یک scatter plot از پیش‌بینی‌ها در برابر مقادیر واقعی رسم می‌کند. اگر use_swa فعال باشد، از SWA model استفاده می‌کند.\n",
        "\n",
        "model به device منتقل و به حالت eval می‌رود. برای هر batch در test_loader، پیش‌بینی‌ها (با یا بدون TTA) محاسبه می‌شوند، به مقیاس اصلی (با max_rul) برمی‌گردند، و در listهای preds و trues ذخیره می‌شوند. metrics با sklearn محاسبه و چاپ می‌شوند، و scatter plot رسم می‌شود.\n",
        "\n",
        "model بارگذاری می‌شود (SWA یا best model)، test data در batchها پردازش می‌شود، پیش‌بینی‌ها جمع‌آوری و denormalize می‌شوند، metrics محاسبه و visualization انجام می‌شود.\n",
        "\n",
        "این تابع performance واقعی model را روی unseen data ارزیابی می‌کند. metrics مختلف جنبه‌های مختلف خطا را پوشش می‌دهند (مثل MSE برای squared error، sMAPE برای relative error). scatter plot به تحلیل بصری خطاها کمک می‌کند. استفاده از SWA به دلیل generalization بهتر انتخاب شده است.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_history(history):\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history[\"train\"], label=\"Train Loss\", color='blue', lw=2)\n",
        "    plt.plot(history[\"val\"], label=\"Validation Loss\", color='orange', lw=2)\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.title(\"Training & Validation Loss\")\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history[\"val_smape\"], label=\"Validation sMAPE%\", color='green', lw=2)\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"sMAPE %\")\n",
        "    plt.title(\"Validation sMAPE Over Time\")\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div dir=\"rtl\">\n",
        "\n",
        "## Plot History\n",
        "\n",
        "این تابع history مربوط به train loss، validation loss، و validation sMAPE را در دو subplot رسم می‌کند.\n",
        "\n",
        "با matplotlib یک figure با دو subplot ایجاد می‌شود. train و val loss در subplot اول و val sMAPE در subplot دوم plot می‌شوند. labelها، grid، و legend اضافه می‌شوند.\n",
        "\n",
        "figure initialize می‌شود، data از history استخراج و plot می‌شود، تنظیمات visualization (مثل title و label) اعمال می‌شوند، و plot نمایش داده می‌شود.\n",
        "\n",
        "visualization از history کمک می‌کند تا روند train را monitor کنیم، convergence را بررسی کنیم، و علائم overfitting (مثل افزایش val loss) را تشخیص دهیم. sMAPE به‌عنوان metric نسبی برای تحلیل خطای relative مفید است.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run():\n",
        "    if not (CACHE_DIR / \"X_raw.npy\").exists():\n",
        "        files = get_all_files(BASE_DIR)\n",
        "        if not files:\n",
        "            raise RuntimeError(\"reversed not found\")\n",
        "        preprocess_and_cache(files)\n",
        "\n",
        "    X_raw, y, max_rul = load_cached_raw()\n",
        "    train_idx, val_idx, test_idx = split_indices(len(X_raw))\n",
        "    mean, std = compute_train_norm_stats(X_raw[train_idx])\n",
        "\n",
        "    train_ds = BearingDataset(X_raw[train_idx], y[train_idx], max_rul, mean, std, train=True)\n",
        "    val_ds = BearingDataset(X_raw[val_idx], y[val_idx], max_rul, mean, std)\n",
        "    test_ds = BearingDataset(X_raw[test_idx], y[test_idx], max_rul, mean, std)\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
        "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, num_workers=0)\n",
        "    test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, num_workers=0)\n",
        "\n",
        "    model = OptimizedModel(TARGET_LEN)\n",
        "    history, swa_model = fit(model, train_loader, val_loader, EPOCHS, LR, MIN_LR, WARMUP_EPOCHS, WEIGHT_DECAY, PATIENCE, device, SWA_START)\n",
        "    plot_history(history)\n",
        "\n",
        "    print(\"\\n=== Final Evaluation with SWA Model ===\")\n",
        "    evaluate_test(model, test_loader, max_rul, use_swa=True, swa_model=swa_model)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div dir=\"rtl\">\n",
        "\n",
        "## Main Run\n",
        "\n",
        "تابع run کل pipeline را اجرا می‌کند: بررسی و اجرای preprocess اگر cache وجود نداشته باشد، load data، تقسیم به train/val/test، ساخت datasetها و DataLoaderها، initialize و train مدل، رسم history، و evaluate نهایی با SWA model.\n",
        "\n",
        "ابتدا بررسی می‌شود که cache وجود دارد یا خیر. اگر نه، preprocess_and_cache اجرا می‌شود. سپس data load و split می‌شود، datasetها و loaderها ساخته می‌شوند، model initialize و train می‌شود، history رسم و evaluate نهایی انجام می‌شود.\n",
        "\n",
        "cache check می‌شود، data preprocess یا load می‌شود، indices تقسیم، datasetها و loaderها ساخته، model train شده، history visualize و test evaluate می‌شود.\n",
        "\n",
        "این تابع کل workflow را به‌صورت یکپارچه مدیریت می‌کند تا اجرای کد آسان باشد. SWA در evaluate نهایی استفاده می‌شود چون معمولاً generalization بهتری نسبت به best model دارد. این ساختار modular امکان debug و تغییر را آسان می‌کند.\n",
        "### نتایج تست\n",
        "- **معیارها** (با مدل SWA و RUL_CLIP=۱۳۰):\n",
        "  - MSE: ۱۱۸.۸۰\n",
        "  - MAE: ۳.۵۳\n",
        "  - RMSE: ۱۰.۹۰\n",
        "  - R²: ۰.۹۰۸۲\n",
        "  - sMAPE: ۵.۸۳٪\n",
        "  - MAE%Max: ۲.۷۱٪\n",
        "</div>"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
